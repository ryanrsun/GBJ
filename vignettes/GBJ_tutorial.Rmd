---
title: "Generalized Berk-Jones (GBJ) Tutorial"
author: "Ryan Sun"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The GBJ package implements the Generalized Berk-Jones (GBJ) test as detailed in:

> "The Generalized Berk-Jones Statistic for Genetic Association Tests, Ryan Sun and Xihong Lin, *in preparation*"

Also included in this package are routines to perform the Generalized Higher Criticism (GHC), Higher Criticism (HC),
Berk-Jones (BJ), and Minimum p-value (minP) tests.  Some guidance on choosing between these methods (which, in principal,
test the same null hypothesis and could be used interchangeably) is also given in the above paper.  In general, researchers
will be interested in these tests when they are interested in possible associations between **one outcome** 
and **many factors**.

The remainder of this vignette provides:

* A high-level explanation of the problems that GBJ/GHC/HC/BJ/minP can solve.
* A worked example in the context of Genome-Wide Association Studies (GWAS). 
* Some specific advice for more specialized cases.

## Uses for GBJ

We find it instructional to begin with a short anecdote from the class notes of John Tukey (Donoho and Jin, 2004):

> "A young psychologist administers many hypothesis tests as part of a research project, and finds that, of 250 tests 11 were significant at the 5% level.  The young researcher feels very proud of this fact and is ready to make a big deal about it, until a senior research suggests that one would expect 12.5 significant tests even in the purely null case, merely by chance.  In that sense, finding only 11 significant results is actually somewhat disappointing! ... [Tukey] then proposed a sort of *second-level significance testing*, ... [to] indicate a kind of *significance of the overall body of tests*."

All the tests performed in this package are designed to carry out the sort of second-level significance testing suggested in the story.  That is, they are designed to test if there is at least one non-null hypothesis in the entire group of hypotheses.  In statistical terms, assume that the researcher above had calculated 250 Z-scores, where $Z_{i}$ has mean $\mu_{i}$ and variance 1.  Then the GBJ null hypothesis is that $\mu_{i}=0$ for all $i$, and the GBJ alternative is that $\mu_{i} \neq 0$ for at least one $i$.

GBJ can be used to test either an entire collection of hypotheses, or it may make more sense to partition the group into
smaller, predefined sets and then apply GBJ multiple times.  For example, in the case of GWAS, we can group the individual SNP test statistics into genes/pathways and apply GBJ to each gene or pathway.  In this case, GBJ is testing if the entire gene has any association with the outcome.

Notable advantages to using GBJ are: 


1. A statistically justified and parametric approach to combining sets of correlated test statistics.  This may provide additional power over more ad-hoc or nonparametric procedures that use count or rank procedures.
2. Application is straightfoward. No tuning parameters to select or 'special cases' that require extra wrangling.  It is reasonable to apply GBJ on sets ranging in size from 2 to ~950 factors. 

    + Theoretically there is no upper limit, but we are currently bound by the numerical precision in standard C++ routines.  There are plans to remove this obstacle through arbitrary precision libraries.
  
3. Analytic calculation of p-values is fast.  There is no need for permutation or other computationally expensive procedures to perform inference.

4.  Output is a p-value, which can be interpreted both as a stand-alone measure or as a competitive ranking, for example to rank pathways in pathway analysis.


## Worked Example

Suppose we are interested in testing whether a specific gene is associated with pancreatic cancer.  We have 200 patients in our study, half with pancreatic cancer and half without.  Our dataset consists of 50 SNPs in the gene of interest, and for each patient we have their minor allele count (0,1,2) at each of the 50 SNPs. Additionally we have information on each patient's age and gender:

```{r}
library(GBJ)
set.seed(1000)
cancer_status <- c(rep(1,100), rep(0,100))
# All of our SNPs have minor allele frequency of 0.3 in this example
genotype_data <- matrix(data=rbinom(n=200*50, size=2, prob=0.3), nrow=200)
age <- round( runif(n=200, min=30, max=80) )
gender <- rbinom(n=200, size=1, prob=0.5)     # Let 1 denote a female and 0 a male
```

Under the null hypothesis of no association between gene and pancreatic cancer, we can assume the true logistic model to be:

$$\text{logit}(\mu_{i}) = \beta_{0} + \beta_{1}*Age_{i} + \beta_{2}*Gender_{i}$$

(actually since we generated the data, we know $\beta_{1}=\beta_{2}=0$, but this is just for illustration)

The function `calc_score_stats()` can be used to calculate score statistics for each of the 50 SNPs.  Under the null, these statistics will have an asymptotic N(0,1) distribution.  If our outcome was continuous and we assumed a linear regression model, then we could still use `calc_score_stats()` with `link_function='linear'`, or if the outcome was non-negative count data and we assume a Poisson regression model, then use `link_function='log'`.  Otherwise, if we wish to assume a more unique model, the user will have to calculate their own test statistics (should be asymptotically N(0,1) under the null) for each SNP.

```{r}
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, link_function="logistic")
log_reg_stats$test_stats
log_reg_stats$cor_mat[1:5,1:5]
```

Now we have both (1) the test statistics and (2) their correlation matrix, so we can apply GBJ or any of the other set-based methods provided by this package. (If starting from summary-level data, begin at this step - simple, right?).  Each test function returns the statistic and then the p-value of that statistic.

```{r}
cor_Z <- log_reg_stats$cor_mat
score_stats = log_reg_stats$test_stats
GBJ(test_stats=score_stats, cor_mat=cor_Z)
GHC(test_stats=score_stats, cor_mat=cor_Z)
HC(test_stats=score_stats, cor_mat=cor_Z)
BJ(test_stats=score_stats, cor_mat=cor_Z)
minP(test_stats=score_stats, cor_mat=cor_Z)
```

## Some advice for special cases

- Note that in the above example we know each of our factors (SNPs) are independent, so another option would be to just input a correlation matrix where all the off-diagonal elements are zero.  This should not drastically change the results since our estimated correlations are so close to 0 anyway. 

- To estimate the correlation vector when starting with summary statistics, we have found that a close approximation is given by the correlations of the factors themselves.  For example, suppose we have genetic consortium summary statistics.  We can navigate to the 1000 Genomes website, download the genotype data at each of the SNPs being tested by GBJ (making sure to only use subjects from an ethnicity close to the one studied by the consortium), and then take the sample correlations between the SNPs.  These sample correlations can then be used in the correlation matrix.

- Sometimes test statistics will have an asymptotic distribution that is not N(0,1) under the null.  Generally these statistics can be transformed to p-values, which can then be transformed into standard normal Z-statistics. In transforming from p-values to Z-statistics, remember that GBJ does not distinguish between positive and negative statistics, so an appropriate transformation is Z=qnorm(1-p_value/2).  

Questions or novel applications? Please let me know!  Contact information can be found in the package description.
