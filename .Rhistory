dim(GSE12470)
numSamples <- NULL
for(i in 1:length(esets)){
numSamples <- c(numSamples, length(sampleNames(esets[[i]])))
}
numSamples
SampleNumberSummaryAll <- data.frame(NumberOfSamples = numSamples, row.names = names(esets))
total <- sum(SampleNumberSummaryAll[,"NumberOfSamples"])
SampleNumberSummaryAll <- rbind(SampleNumberSummaryAll, total)
rownames(SampleNumberSummaryAll)[nrow(SampleNumberSummaryAll)] <- "Total"
table(SampleNumberSummaryAll)
pDataID <- c("sample_type", "histological_type", "primarysite", "summarygrade", "summarystage",
"tumorstage", "grade", "age_at_initial_pathologic_diagnosis", "pltx", "tax", "neo",
"days_to_tumor_recurrence", "recurrence_status", "days_to_death", "vital_status")
pDataPercentSummaryTable <- NULL
pDataSummaryNumbersTable <- NULL
for(e in 1:length(esets)){
eset <- esets[[e]]
pDataPercentSummary <- NULL
pDataSummaryNumbers <- NULL
for(p in 1:length(pDataID)){
pDataSummaryNumbers <- c(pDataSummaryNumbers,
sum(!is.na(pData(eset)[,pDataID[p]])))
pDataPercentSummary <- c(pDataPercentSummary,
(sum(!is.na(pData(eset)[,pDataID[p]]))/nrow(pData(eset)))*100)
}
if(e == 1){
pDataSummaryNumbersTable <- data.frame(test = pDataSummaryNumbers)
pDataPercentSummaryTable <- data.frame(test = pDataPercentSummary)
} else {
pDataPercentSummaryTable <- cbind(pDataPercentSummaryTable,pDataPercentSummary)
pDataSummaryNumbersTable <- cbind(pDataSummaryNumbersTable, pDataSummaryNumbers)
}
}
rownames(pDataSummaryNumbersTable) <- pDataID
rownames(pDataPercentSummaryTable) <- pDataID
colnames(pDataSummaryNumbersTable) <- names(esets)
colnames(pDataPercentSummaryTable) <- names(esets)
pDataSummaryNumbersTable <- rbind(pDataSummaryNumbersTable, total)
rownames(pDataSummaryNumbersTable)[nrow(pDataSummaryNumbersTable)] <- "Total"
# Generate a heatmap representation of the pData
pDataPercentSummaryTable<-t(pDataPercentSummaryTable)
pDataPercentSummaryTable<-cbind(Name=(rownames(pDataPercentSummaryTable)),pDataPercentSummaryTable)
nba<-pDataPercentSummaryTable
gradient_colors = c("#ffffff","#ffffd9","#edf8b1","#c7e9b4","#7fcdbb",
"#41b6c4","#1d91c0","#225ea8","#253494","#081d58")
library(lattice)
nbamat<-as.matrix(nba)
rownames(nbamat)<-nbamat[,1]
nbamat<-nbamat[,-1]
Interval<-as.numeric(c(10,20,30,40,50,60,70,80,90,100))
levelplot(t(nbamat),col.regions=gradient_colors,main="Available Clinical Annotation",
scales=list(x=list(rot=90, cex=0.5), y= list(cex=0.5),key=list(cex=0.2)),
at=seq(from=0,to=100,length=10),cex=0.2, ylab="", xlab="", lattice.options=list(),
colorkey=list(at=as.numeric(factor(c(seq(from=0, to=100, by=10)))),
labels=as.character(c( "0","10%","20%","30%", "40%","50%",
"60%", "70%", "80%","90%", "100%"),
cex=0.2,font=1,col="brown",height=1, width=1.4),
col=(gradient_colors)))
head(GSE12470)
?GSE12470
?GSE12470_eset
data(GSE12470_eset)
data( GSE12470_eset )
GSE12470
GSE12470_eset
esets
esets[[1]]
ell = esets[[1]]
pData(ell)
names(pdata(ell))
names(pData(ell))
pData(ell)$sample_type
pData(ell)$days_to_death
pData(ell)$unique_patient_ID
sData(ell)
names(ell)
ell
fData(ell)
head(fData(ell))
names(fData(ell))
ell
experimentData(ell)
aData(ell)
exprs(ell)
head(exprs(ell))
gene_sets
# We need a list of sets
set_names <- paste('Set', 1:1300, sep='')
gene_lists <- matrix(data=1:13000, nrow=1300, ncol=10)
gene_sets <- data.frame(set_name=set_names)
gene_sets <- cbind(gene_sets, gene_lists)
head(gene_sets)
# Run the analysis on ER-/HER2- cohort first, about 500 patients
mm_ind <- which(st=='ER-/HER2-')
length(mm_ind)
dat_mm <- dat.st[mm_ind,]
time_mm <- time[mm_ind]
event_mm <- event[mm_ind]
# Now just apply GBJ and GHC to each one
gene_sets <- gene_sets[1:10, ]
GOF_results <- data.frame( matrix(data=NA, nrow=nrow(gene_sets), ncol=6) )
colnames(GOF_results) <- c('Pathway_Name', 'GBJ', 'GBJ_p', 'GHC', 'GHC_p', 'Num_Genes')
load('Breast.Rdata')
setwd('/users/ryansun/downloads')
load('Breast.Rdata')
# Run the analysis on ER-/HER2- cohort first, about 500 patients
mm_ind <- which(st=='ER-/HER2-')
length(mm_ind)
dat_mm <- dat.st[mm_ind,]
time_mm <- time[mm_ind]
event_mm <- event[mm_ind]
GOF_results[,1] <- gene_sets[,1]
GOF_results[,3] <- runif(n=nrow(GOF_results))
GOF_results[,5] <- runif(n=nrow(GOF_results))
names(pData(ell))
table(SampleNumberSummaryAll)
vignette('GBJ_tutorial')
set.seed(1000)
cancer_status <- c(rep(1,100), rep(0,100))
genotype_data <- matrix(data=rbinom(n=200*50, size=2, prob=0.3), nrow=200)
age <- round( runif(n=200, min=30, max=80) )
gender <- rbinom(n=200, size=1, prob=0.5)     # Let 1 denote a female and 0 a male
age
gedner
gender
genotype_data
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, model_type="logistic")
library(GBJ)
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, model_type="logistic")
log_reg_stats
names(pData(ell))
pdata(ell)$histologicaltype
pData(ell)$histological_type
table(SampleNumberSummaryAll)
library(GBJ)
set.seed(1000)
cancer_status <- c(rep(1,100), rep(0,100))
# All of our SNPs have minor allele frequency of 0.3 in this example
genotype_data <- matrix(data=rbinom(n=200*50, size=2, prob=0.3), nrow=200)
age <- round( runif(n=200, min=30, max=80) )
gender <- rbinom(n=200, size=1, prob=0.5)
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, link_function="logistic")
log_reg_stats$test_stats
log_reg_stats$cor_mat[1:5,1:5]
library(GBJ)
set.seed(1000)
cancer_status <- c(rep(1,100), rep(0,100))
# All of our SNPs have minor allele frequency of 0.3 in this example
genotype_data <- matrix(data=rbinom(n=200*50, size=2, prob=0.3), nrow=200)
age <- round( runif(n=200, min=30, max=80) )
gender <- rbinom(n=200, size=1, prob=0.5)
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, link_function="logistic")
log_reg_stats$test_stats
log_reg_stats$cor_mat[1:5,1:5]
null_mod <- glm(cancer_status~age+gender, family=Poisson(link='log'))
null_mod <- glm(cancer_status~age+gender, family=poisson())
summary(null_mod)
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
summary(null_mod)
null_mod <- glm(cancer_status~age+gender, family=poisson())
head(null_mod$fitted.values)
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
head(null_mod$fitted.values)
null_mod <- glm(cancer_status~age+gender, family=poisson())
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, link_function="log")
log_reg_stats$test_stats
log_reg_stats$cor_mat[1:5,1:5]
getwd()
library(roxygen2)
document()
library(devtools)
document()
# Only difference between linear and logistic procedure
if (link_function == 'logistic') {
W_vec <- fitted_Y * (1-fitted_Y)
} else if (link_function == 'linear') {
W_vec <- rep(summary(null_model)$dispersion, nrow(X_mat))
} else if (link_function == 'log') {
W_vec <- fitted_Y
} else {
stop("Invalid model type")
}
W_mat <- diag(W_vec)
P_mat <- W_mat - W_mat%*%X_mat %*% solve(t(X_mat)%*%W_mat%*%X_mat) %*% t(X_mat)%*%W_mat
# Now our score test
d <- ncol(factor_matrix)
test_stats <- rep(NA, d)
denominators <- rep(NA, d)
for(kkk in 1:d)
{
# Pick out next SNP, conduct score test (no additional covariates).
tempF <- factor_matrix[,kkk]
score_num <- t(tempF) %*% (actual_Y-fitted_Y)
score_denom <- sqrt(tempF %*% P_mat %*% tempF)
test_stats[kkk] <- score_num / score_denom
denominators[kkk] <- score_denom
}
getwd()
test_stats = rnorm(n=10) + 0.2
test_Stats
test_stats
test_stats = abs(rnorm(10)) + 0.2
sig_mat = diag(nrow=10)
sig_mat
cor_mat=sig_mat
# Parse inputs, do some error checking.
param_list <- parse_input(test_stats=test_stats, cor_mat=cor_mat,
pairwise_cors=pairwise_cors)
t_vec <- param_list$t_vec
pairwise_cors <- param_list$pairwise_cors
d <- length(t_vec)
library(GBJ)
# Parse inputs, do some error checking.
param_list <- parse_input(test_stats=test_stats, cor_mat=cor_mat,
pairwise_cors=pairwise_cors)
t_vec <- param_list$t_vec
pairwise_cors <- param_list$pairwise_cors
d <- length(t_vec)
pairwise_cors=NULL
# Parse inputs, do some error checking.
param_list <- parse_input(test_stats=test_stats, cor_mat=cor_mat,
pairwise_cors=pairwise_cors)
t_vec <- param_list$t_vec
pairwise_cors <- param_list$pairwise_cors
d <- length(t_vec)
d
t_vec
pairwise_cors
Sometimes test stats are too big for R's precision
too_big <- which(t_vec > 8.2)
if (length(too_big) > 0) {t_vec[too_big] <- 8.2}
# Correct number of pairwise correlations?
if (length(pairwise_cors) != d*(d-1)/2) {
stop("Your pairwise correlation vector is of the wrong length!")
}
GBJ_stats <- GBJ_objective(t_vec=t_vec, d=d, pairwise_cors=pairwise_cors)
GBJ_stats
# Check for qualifying p-values under the null (the indicator part of the GBJ statistic)
# and also that we are only considering 'first half' p-values
p_values <- 1-pchisq(t_vec^2, df=1)
BJ_indicator <- which( p_values < (1:d)/d )
first_half <- 1:(ceiling(d/2))
non_zero <- intersect(BJ_indicator, first_half)
# If no indicies qualified, stop
if (length(non_zero) == 0) {
return ( list(BJ=0, BJ_pvalue=1) )
}
#################################
# Some indicies passed
i_vec <- 1:d
BJ_stats <- rep(0, d)
BJ_stats[non_zero] <- i_vec[non_zero] * log(i_vec[non_zero]/(d*p_values[non_zero])) +
(d-i_vec[non_zero]) * log((1-i_vec[non_zero]/d)/(1-p_values[non_zero]))
BJ_stats[d] <- 0
BJ_stats
max(BJ_stats)
gbj <- max(GBJ_stats)
gbj
GBJ_p_list <- GBJ_pvalue(observed_gbj=gbj, d=d, pairwise_cors=pairwise_cors)
GBJ_p_list
observed_gbj=gbj
gbj
times_to_try=5
times_tried <- 0
eFlag <- 1
eFlag <- 0
# Make gBJ_BB bounds
GBJ_z_bounds <- rep(NA,d)
prev_bound <- 8.2
for ( kkk in 1:(ceiling(d/2)) )
{
temp_gbj <- tryCatch(uniroot(GBJ_objective, interval=c(0, prev_bound), d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj), error=function(e) e, warning=function(w) w)
# Sometimes, we can't go high enough in t, because pnorm,etc will just round to 0, and thus
# the signs on both sides of the interval will be the same.  In this case, we will try again
# a few times and then give up.
if(length(class(temp_gbj))>1) {
eFlag <- 1
times_tried <- times_tried + 1
break
} else {
GBJ_z_bounds[kkk] <- temp_gbj$root
}
prev_bound <- GBJ_z_bounds[kkk]
}
times_tried
GBJ_z_bounds
kkk
temp_gbj
prev_bound
observed_gbj
GBJ_objective(t_vec=0, d=d, k_vec=kkk, pairwise_cors=pairwise_cors)
GBJ_objective(t_vec=0, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=5, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
kkk
GBJ_objective(t_vec=8.2, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=8, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=7, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=6, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=5, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=4, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=3, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
GBJ_objective(t_vec=5.5, d=d, k_vec=kkk, pairwise_cors=pairwise_cors, offset=observed_gbj)
k_vec=1
t_vec=5
# Ensure that the thresholds are sorted in descending order, largest first.
t_vec <- sort(abs(t_vec), decreasing=TRUE)
# If didn't pass in a kkk vector, create it now (assume passed in all test statistics)
if (is.null(k_vec)) {
k_vec <- 1:d
}
# Check for qualifying p-values under the null (the indicator part of the GBJ statistic)
# and also that we are only considering 'first half' p-values
p_values <- 1-pchisq(t_vec^2, df=1)
GBJ_indicator <- which( p_values < k_vec/d )
first_half <- which(k_vec <= ceiling(d/2))
non_zero <- intersect(GBJ_indicator, first_half)
# If no indicies qualified, stop
if (length(non_zero) == 0) {
return (rep(0,length(t_vec)) - rep(offset, length(t_vec)) )
}
k_vec
d
p_values
non_zero
#################################
# Some indices passed
# Calculate mean and variance of S(t) under the null for non-zero t
mean_null <- rep(NA, length(t_vec))
sigsq_null <- rep(NA, length(t_vec))
mean_null[non_zero] <- 2*d*surv(t_vec[non_zero])
sigsq_null <- calc_var_nonzero_mu(d=d, t=t_vec, mu=rep(0,length(t_vec)), pairwise_cors=pairwise_cors)
mean_null
sig_sq_null
sigsq_null
# Now match moments for null (directly)
mu_null <- rep(NA, length(t_vec))
rho_null <- rep(NA, length(t_vec))
gamma_null <- rep(NA, length(t_vec))
mu_null[non_zero] <- mean_null[non_zero] / d
rho_null[non_zero] <- (sigsq_null[non_zero] - d*mu_null[non_zero]*(1-mu_null[non_zero])) /
(d*(d-1)*mu_null[non_zero]*(1-mu_null[non_zero]))
gamma_null[non_zero] = rho_null[non_zero] / (1-rho_null[non_zero])
################################
# Calculate variance of S(t) under the alternative for non-zero t (the mean is given by k_vec)
# First, for each t, need to find the fitted common mean of the underlying test statistics
Z_common_means <- rep(NA, length(t_vec))
for (iii in 1:length(t_vec)) {
if(iii %in% non_zero)
{
# First get us the mean of the Z stats under the alternative
Z_common_means[iii] <- uniroot(qnorm_mu, lower=0, upper=100, t=t_vec[iii], kkk=k_vec[iii], d=d)$root
}
}
Z_common_means
# Now we can calculate the variance
alt_mean_vec <- k_vec
alt_var_vec <- rep(NA, length(t_vec))
alt_var_vec[non_zero] <- calc_var_nonzero_mu(d=d, t=t_vec[non_zero], mu=Z_common_means[non_zero],
pairwise_cors=pairwise_cors)
# Now match moments for alternative
mu_alt <- rep(NA, length(t_vec))
rho_alt <- rep(NA, length(t_vec))
gamma_alt <- rep(NA, length(t_vec))
mu_alt[non_zero] <- alt_mean_vec[non_zero] / d
rho_alt[non_zero] <- (alt_var_vec[non_zero] - d*mu_alt[non_zero]*(1-mu_alt[non_zero])) /
(d*(d-1)*mu_alt[non_zero]*(1-mu_alt[non_zero]))
gamma_alt[non_zero] = rho_alt[non_zero] / (1-rho_alt[non_zero])
# Numerical issues may give us mu and gamma that don't respect the bounds.
# We need gamma >= max{ -mu/(d-1), -(1-mu)/(d-1) }
pq_mat_null <- matrix(data=NA, nrow=length(t_vec), ncol=2)
pq_mat_alt <- matrix(data=NA, nrow=length(t_vec), ncol=2)
pq_mat_null[non_zero, 1] <- -mu_null[non_zero] / (d-1)
pq_mat_null[non_zero, 2] <- -(1-mu_null[non_zero]) / (d-1)
gamma_check_null <- apply(pq_mat_null, 1, max)
pq_mat_alt[non_zero, 1] <- -mu_alt[non_zero] / (d-1)
pq_mat_alt[non_zero, 2] <- -(1-mu_alt[non_zero]) / (d-1)
gamma_check_alt <- apply(pq_mat_alt, 1, max)
# Recalibrate non_zero
non_zero <- which(gamma_null >= gamma_check_null &
gamma_alt >= gamma_check_alt)
if (length(non_zero)==0 ) {
(rep(0,length(t_vec)) - rep(offset, length(t_vec)) )
}
non_zero
gamma_null
gamma_alt
t_vec=7
# Ensure that the thresholds are sorted in descending order, largest first.
t_vec <- sort(abs(t_vec), decreasing=TRUE)
# If didn't pass in a kkk vector, create it now (assume passed in all test statistics)
if (is.null(k_vec)) {
k_vec <- 1:d
}
# Check for qualifying p-values under the null (the indicator part of the GBJ statistic)
# and also that we are only considering 'first half' p-values
p_values <- 1-pchisq(t_vec^2, df=1)
GBJ_indicator <- which( p_values < k_vec/d )
first_half <- which(k_vec <= ceiling(d/2))
non_zero <- intersect(GBJ_indicator, first_half)
non_zero
# If no indicies qualified, stop
if (length(non_zero) == 0) {
return (rep(0,length(t_vec)) - rep(offset, length(t_vec)) )
}
#################################
# Some indices passed
# Calculate mean and variance of S(t) under the null for non-zero t
mean_null <- rep(NA, length(t_vec))
sigsq_null <- rep(NA, length(t_vec))
mean_null[non_zero] <- 2*d*surv(t_vec[non_zero])
sigsq_null <- calc_var_nonzero_mu(d=d, t=t_vec, mu=rep(0,length(t_vec)), pairwise_cors=pairwise_cors)
# Now match moments for null (directly)
mu_null <- rep(NA, length(t_vec))
rho_null <- rep(NA, length(t_vec))
gamma_null <- rep(NA, length(t_vec))
mu_null[non_zero] <- mean_null[non_zero] / d
rho_null[non_zero] <- (sigsq_null[non_zero] - d*mu_null[non_zero]*(1-mu_null[non_zero])) /
(d*(d-1)*mu_null[non_zero]*(1-mu_null[non_zero]))
gamma_null[non_zero] = rho_null[non_zero] / (1-rho_null[non_zero])
mean_null
sigsq_null
################################
# Calculate variance of S(t) under the alternative for non-zero t (the mean is given by k_vec)
# First, for each t, need to find the fitted common mean of the underlying test statistics
Z_common_means <- rep(NA, length(t_vec))
for (iii in 1:length(t_vec)) {
if(iii %in% non_zero)
{
# First get us the mean of the Z stats under the alternative
Z_common_means[iii] <- uniroot(qnorm_mu, lower=0, upper=100, t=t_vec[iii], kkk=k_vec[iii], d=d)$root
}
}
# Now we can calculate the variance
alt_mean_vec <- k_vec
alt_var_vec <- rep(NA, length(t_vec))
alt_var_vec[non_zero] <- calc_var_nonzero_mu(d=d, t=t_vec[non_zero], mu=Z_common_means[non_zero],
pairwise_cors=pairwise_cors)
# Now match moments for alternative
mu_alt <- rep(NA, length(t_vec))
rho_alt <- rep(NA, length(t_vec))
gamma_alt <- rep(NA, length(t_vec))
mu_alt[non_zero] <- alt_mean_vec[non_zero] / d
rho_alt[non_zero] <- (alt_var_vec[non_zero] - d*mu_alt[non_zero]*(1-mu_alt[non_zero])) /
(d*(d-1)*mu_alt[non_zero]*(1-mu_alt[non_zero]))
gamma_alt[non_zero] = rho_alt[non_zero] / (1-rho_alt[non_zero])
# Numerical issues may give us mu and gamma that don't respect the bounds.
# We need gamma >= max{ -mu/(d-1), -(1-mu)/(d-1) }
pq_mat_null <- matrix(data=NA, nrow=length(t_vec), ncol=2)
pq_mat_alt <- matrix(data=NA, nrow=length(t_vec), ncol=2)
pq_mat_null[non_zero, 1] <- -mu_null[non_zero] / (d-1)
pq_mat_null[non_zero, 2] <- -(1-mu_null[non_zero]) / (d-1)
gamma_check_null <- apply(pq_mat_null, 1, max)
pq_mat_alt[non_zero, 1] <- -mu_alt[non_zero] / (d-1)
pq_mat_alt[non_zero, 2] <- -(1-mu_alt[non_zero]) / (d-1)
gamma_check_alt <- apply(pq_mat_alt, 1, max)
# Recalibrate non_zero
non_zero <- which(gamma_null >= gamma_check_null &
gamma_alt >= gamma_check_alt)
if (length(non_zero)==0 ) {
(rep(0,length(t_vec)) - rep(offset, length(t_vec)) )
}
non_zero
length(t_vec)
# Log-liklihood for null
null_loglik <- rep(0, length(t_vec))
null_param_mat <- cbind( k_vec[non_zero], mu_null[non_zero], gamma_null[non_zero])
null_loglik[non_zero] <- apply(null_param_mat, 1, ebb_loglik, d=d)
# Log-likelihood for alternative
alt_loglik <- rep(0, length(t_vec))
alt_param_mat <- cbind( k_vec[non_zero], mu_alt[non_zero], gamma_alt[non_zero])
alt_loglik[non_zero] <- apply(alt_param_mat, 1, ebb_loglik, d=d)
BB_GBJ_stats <- alt_loglik - null_loglik
BB_GBJ_stats
alt_loglik
gamma_null
gamma_check_null
gamma_alt
gamma_check_alt
setwd('/users/ryansun/desktop')
install.packages(repos=NULL, pkgs='GBJ_0.1.0.tar.gz', type='source')
test_stats
GBJ(test_stats, cor_mat)
library(GBJ)
GBJ(test_stats, cor_mat)
library(GBJ)
test_stats = abs(rnorm(10)) + 0.2
cor_mat=diag(nrow=10)
GBJ(test_stats, cor_mat)
pairwise_cors=NULL
param_list <- parse_input(test_stats=test_stats, cor_mat=cor_mat,
pairwise_cors=pairwise_cors)
t_vec <- param_list$t_vec
pairwise_cors <- param_list$pairwise_cors
d <- length(t_vec)
# Sometimes test stats are too big for R's precision
too_big <- which(t_vec > 8.2)
if (length(too_big) > 0) {t_vec[too_big] <- 8.2}
# Correct number of pairwise correlations?
if (length(pairwise_cors) != d*(d-1)/2) {
stop("Your pairwise correlation vector is of the wrong length!")
}
# Calculate the observed GBJ statistic
GBJ_stats <- GBJ_objective(t_vec=t_vec, d=d, pairwise_cors=pairwise_cors)
gbj <- max(GBJ_stats)
gbj
observed_gbj=gbj
setwd('/users/ryansun/desktop')
install.packages(repos=NULL, pkgs='GBJ_0.1.0.tar.gz', type='source')
