G <- rbinom(n=n, size=2, prob=mu_G)
eta <- beta_0 + G*beta_G
mu <- expit(eta)
Y <- rbinom(n=n, size=1, prob=mu)
# If not enough cases/controls
if (sum(Y) < total_cases | sum(Y) > (n-total_controls)) {next}
# Pick out cases and controls for each cohort
all_cases <- which(Y==1)
all_controls <- which(Y==0)
disc_rows <- c(all_cases[1:num_cases1], all_controls[1:num_controls1])
v1_rows <- c(all_cases[(num_cases1+1):(num_cases1+num_cases2)],
all_controls[(num_controls1+1):(num_controls1+num_controls2)])
v2_rows <- c(all_cases[(num_cases1+num_cases2+1):(num_cases1+num_cases2+num_cases3)],
all_controls[(num_controls1+num_controls2+1):(num_controls1+num_controls2+num_controls3)])
disc_design <- cbind(1, G[disc_rows])
disc_Y <- Y[disc_rows]
v1_design <- cbind(1, G[v1_rows])
v1_Y <- Y[v1_rows]
v2_design <- cbind(1, G[v2_rows])
v2_Y <- Y[v2_rows]
mod_disc <- speedglm.wfit(y=disc_Y, X=disc_design, family=binomial(link='logit'))
mod_v1 <- speedglm.wfit(y=v1_Y, X=v1_design, family=binomial(link='logit'))
mod_v2 <- speedglm.wfit(y=v2_Y, X=v2_design, family=binomial(link='logit'))
fitted_disc <- as.numeric(expit(disc_design %*% mod_disc$coefficients))
bread_disc <- solve( crossprod( disc_design * fitted_disc*(1-fitted_disc), disc_design ) )
fitted_v1 <- as.numeric(expit(v1_design %*% mod_v1$coefficients))
bread_v1 <- solve( crossprod( v1_design * fitted_v1*(1-fitted_v1), v1_design ) )
fitted_v2 <- as.numeric(expit(v2_design %*% mod_v2$coefficients))
bread_v2 <- solve( crossprod( v2_design * fitted_v2*(1-fitted_v2), v2_design ) )
disc_stat <- mod_disc$coefficients[2] / sqrt(bread_disc[2,2])
v1_stat <- mod_v1$coefficients[2] / sqrt(bread_v1[2,2])
v2_stat <- mod_v2$coefficients[2] / sqrt(bread_v2[2,2])
# Record
record_row <- (beta_G_it-1) * num_sims + i
ARDS_gwas[record_row, 1] <- beta_G
ARDS_gwas[record_row, 2] <- 1-pchisq(disc_stat^2, df=1)
ARDS_gwas[record_row, 3] <- 1-pchisq(v1_stat^2, df=1)
ARDS_gwas[record_row, 4] <- 1-pchisq(v2_stat^2, df=1)
disc_G <- G[disc_rows]
v1_G <- G[v1_rows]
v2_G <- G[v2_rows]
disc_mod <- glm(disc_Y ~ disc_G, family=binomial(link='logit'))
v1_mod <- glm(v1_Y ~ v1_G, family=binomial(link='logit'))
v2_mod <- glm(v2_Y ~ v2_G, family=binomial(link='logit'))
record_row <- (beta_G_it-1) * num_sims + i
validation[record_row, 1] <- beta_G
validation[record_row, 2] <- summary(disc_mod)$coefficients[2,4]
validation[record_row, 3] <- summary(v1_mod)$coefficients[2,4]
validation[record_row, 4] <- summary(v2_mod)$coefficients[2,4]
if (i %% 1000 == 0) {cat(i)}
}
end = proc.time()
end - start
start = proc.time()
for (i in 1:num_sims) {
# Generate 1 covariate and outcome
G <- rbinom(n=n, size=2, prob=mu_G)
eta <- beta_0 + G*beta_G
mu <- expit(eta)
Y <- rbinom(n=n, size=1, prob=mu)
# If not enough cases/controls
if (sum(Y) < total_cases | sum(Y) > (n-total_controls)) {next}
# Pick out cases and controls for each cohort
all_cases <- which(Y==1)
all_controls <- which(Y==0)
disc_rows <- c(all_cases[1:num_cases1], all_controls[1:num_controls1])
v1_rows <- c(all_cases[(num_cases1+1):(num_cases1+num_cases2)],
all_controls[(num_controls1+1):(num_controls1+num_controls2)])
v2_rows <- c(all_cases[(num_cases1+num_cases2+1):(num_cases1+num_cases2+num_cases3)],
all_controls[(num_controls1+num_controls2+1):(num_controls1+num_controls2+num_controls3)])
if (i %% 1000 == 0) {cat(i)}
}
end = proc.time()
end - start
n
# Number of total subjects we have to generate to get enough cases and controls
n <- round(total_cases * 2 / expit(beta_0), digits=0)
start = proc.time()
for (i in 1:num_sims) {
# Generate 1 covariate and outcome
G <- rbinom(n=n, size=2, prob=mu_G)
eta <- beta_0 + G*beta_G
mu <- expit(eta)
Y <- rbinom(n=n, size=1, prob=mu)
# If not enough cases/controls
if (sum(Y) < total_cases | sum(Y) > (n-total_controls)) {next}
# Pick out cases and controls for each cohort
all_cases <- which(Y==1)
all_controls <- which(Y==0)
disc_rows <- c(all_cases[1:num_cases1], all_controls[1:num_controls1])
v1_rows <- c(all_cases[(num_cases1+1):(num_cases1+num_cases2)],
all_controls[(num_controls1+1):(num_controls1+num_controls2)])
v2_rows <- c(all_cases[(num_cases1+num_cases2+1):(num_cases1+num_cases2+num_cases3)],
all_controls[(num_controls1+num_controls2+1):(num_controls1+num_controls2+num_controls3)])
if (i %% 1000 == 0) {cat(i)}
}
end = proc.time()
end - start
######################################################################################################
# Simulations for SNP-set setting
K <- 100
rho <- 0.3
alpha <- 0.05
num_VC_null <- 100000
num_PC_sims <- 100000
num_GOF <- 100
min_causal <- 1
max_causal <- 10
library(mvnfast)
mult_pheno_exch <- matrix(data=NA, nrow=(max_causal-min_causal+1), ncol=8)
for (num_causal in min_causal:max_causal)
{
# Build exchangeable covariance matrix
sig_mat_exch <- matrix(data=rho, nrow=K, ncol=K)
diag(sig_mat_exch) <- 1
# Eigenvectors and eigenvalues
SM_eigen <- eigen(sig_mat_exch)
U <- SM_eigen$vectors
Lambda <- SM_eigen$values
# Set R^2=0.01.  Since variance of Y is always 1 in derivations,
# take beta=\sqrt(0.01/num_causal)
#beta_1 <- sqrt(1 / num_causal)
beta_1 <- 2
beta_vec <- c( rep(beta_1, num_causal), rep(0, (K-num_causal)) )
# Estimate the null distribution of the VC test.
# Use rmvn from mvnfast package, otherwise takes too long to do 100000.
Z_null <- rmvn(n=num_VC_null, mu=rep(0,K), sigma=sig_mat_exch, ncores=1)
temp_inv <- solve(sig_mat_exch)
VC_null <- rep(0, num_VC_null)
for (j in 1:num_VC_null) {
VC_null[j] <- (Z_null[j,] %*% temp_inv %*% temp_inv %*% Z_null[j, ])^2
}
VC_crit_val <- quantile(VC_null, 1-alpha)
# Now simulate the test statistics under the alternative
Z_alt <- rmvn(n=num_PC_sims, mu=beta_vec, sigma=sig_mat_exch)
# Testing on PCs
all_PCs <- Z_alt %*% U
reject_pc1 <- which(abs( all_PCs[,1]/sqrt(Lambda[1]) ) > qnorm(1-alpha/2))
reject_pc2 <- which(abs( all_PCs[,2]/sqrt(Lambda[2]) ) > qnorm(1-alpha/2))
reject_pcK <- which(abs( all_PCs[,K]/sqrt(Lambda[K]) ) > qnorm(1-alpha/2))
# Testing for VC
VC_stats <- rep(0, num_PC_sims)
for (j in 1:num_PC_sims) {
VC_stats[j] <- (Z_alt[j,] %*% temp_inv %*% temp_inv %*% Z_alt[j, ])^2
}
# Record
mult_pheno_exch[num_causal, 1] <- length(reject_pc1) / num_PC_sims
mult_pheno_exch[num_causal, 2] <- length(reject_pc2) / num_PC_sims
mult_pheno_exch[num_causal, 3] <- length(reject_pcK) / num_PC_sims
mult_pheno_exch[num_causal, 4] <- length(which(VC_stats > VC_crit_val)) / num_PC_sims
cat(num_causal)
}
colnames(mult_pheno_exch) <- c('PC1', 'PC2', 'PCK', 'VC', 'GBJ', 'GHC', 'BJ', 'HC')
mult_pheno_exch
num_PC_sims
data("heptathlon")
data("heptathlon", package="HSAUR")
install.packages("HSAUR")
data("heptathlon", package="HSAUR")
heptathlon
heptathlon$hurdles <- max(heptathlon$hurdles - heptathlon$hurdles)
heptathlon
data("heptathlon", package="HSAUR")
heptathlon$hurdles <- max(heptathlon$hurdles) - heptathlon$hurdles
heptathlon
heptathlon$run200m <- max(heptathlon$run200m) -
heptathlon$run200m
heptathlon$run800m <- max(heptathlon$run800m) -
heptathlon$run800m
hepthathlon
heptathlon
round(cor(heptathlon[,-score]), 2)
round(cor(heptathlon[,-8]), 2)
heptathlon_pca <- prcomp(heptathlon[, -8], scale = TRUE)
print(heptathlon_ca)
print(heptathlon_pca)
summary(heptathlon_pca)
a1 <- heptathlon_pca$rotation[,1]
a1
center <- heptathlon_pca$center
center
apply(heptathlon_pca$rotation, 2, mean)
apply(heptathlon, 2, mean)
scale <- heptathlon_pca$scale
hm <- as.matrix(heptathlon[,-8])
drop(scale(hm, center = center, scale = scale)
scale(hm, center = center, scale = scale)
ell = scale(hm, center = center, scale = scale)
ell
apply(ell, 2, mean)
apply(ell, 2, sd)
ell %*% heptathlon_pca$rotation[,1]
predict(heptathlon_pca)[,]
predict(heptathlon_pca)[,1]
head(pheno_PCs$rotation[,1])
head(pheno_PCs$rotation[,1])
ell
heptathlon_pca
ell
10000/60
library(mvnfast)
library(GBJ)
Snum=1
aID=1
set.seed(100 + Snum + aID)
######################################################################################################
# Simulations for SNP-set setting
K <- 100
rho <- 0.3
alpha <- 0.05
num_GOF <- 100
num_sub <- 1000
num_VC_null <- 10000
min_causal <- 1
max_causal <- 10
########################################################
# Do simulation with exchangeable correlation.
# This is the K:1 multiple phenotype situation.
# Vary the number of causal phenotypes from 1-10, total number is 100.
# Simulate from the test statistics, not the individual genotypes.
# We expect that GBJ/GHC will perform quite well here because the signals are
# still going to be very sparse (up to 100^-0.5).
# HC/BJ will probably not perform very well because the signals in the eigenvectors will be dense.
# VC will not perform very well with sparse signal.
# PC1 and PCK should have roughly equal means (not clear if this is true in general)
# and so PCK is preferred in multiple phenotype setting (since it has much smaller variance).
# Actually we would expect PC2 to be very powerful here because it has the same mean and
# the same variance as PCK.
mult_pheno_exch <- matrix(data=NA, nrow=(max_causal-min_causal+1)*num_GOF, ncol=10)
num_causal = 8
sig_mat_exch <- matrix(data=rho, nrow=K, ncol=K)
diag(sig_mat_exch) <- 1
# Beta_vec
beta_1 <- 4
beta_vec <- c( rep(beta_1, num_causal), rep(0, (K-num_causal)) )
# Estimate the null distribution of the VC test.
# Use rmvn from mvnfast package, otherwise takes too long to do 10000.
Z_null <- rmvn(n=num_VC_null, mu=rep(0,K), sigma=sig_mat_exch, ncores=1)
temp_inv <- solve(sig_mat_exch)
VC_null <- rep(0, num_VC_null)
for (j in 1:num_VC_null) {
VC_null[j] <- (Z_null[j,] %*% temp_inv %*% temp_inv %*% Z_null[j, ])^2
}
VC_crit_val <- quantile(VC_null, 1-alpha)
num_GOF
mult_pheno_exch <- matrix(data=NA, nrow=(max_causal-min_causal+1)*num_GOF, ncol=10)
dim(mult_pheno_exch)
50*10
num_GOF <- 50
mult_pheno_exch <- matrix(data=NA, nrow=(max_causal-min_causal+1)*num_GOF, ncol=10)
G_vec <- rbinom(n=num_sub, size=2, prob=0.3)
# Center and scale G so that sum(G_i^2) =1
G_vec <- ( G_vec-mean(G_vec) )
sum_G2 <- sum(G_vec^2)
G_vec <- G_vec / sqrt(sum_G2)
# Then generate Y
Y_mat <- matrix(data=NA, nrow=num_sub, ncol=K)
for (i in 1:num_sub) {
Y_mat[i, ] <- rmvn(n=1, mu=(beta_vec*G_vec[i]), sigma=sig_mat_exch)
}
# Then standardize Y
for (i in 1:ncol(Y_mat)) {
temp_col <- Y_mat[, i]
Y_mat[, i] <-  (temp_col - mean(temp_col)) / sd(temp_col)
}
# Now build our test statistics
# A score test is t(G) %*% Y
test_stats <- t(G_vec) %*% Y_mat
# Eigenvectors and eigenvalues
Y_sig <- cor(Y_mat)
SM_eigen <- eigen(Y_sig)
U <- SM_eigen$vectors
Lambda <- SM_eigen$values
# Testing on PCs
all_PC_test_stats <- test_stats %*% U
PC_test_stats_stan <- all_PC_test_stats / sqrt(Lambda)
pc1_pvalue <- 1-pchisq(PC_test_stats_stan[1]^2, df=1)
pc2_pvalue <- 1-pchisq(PC_test_stats_stan[2]^2, df=1)
pcK_pvalue <- 1-pchisq(PC_test_stats_stan[K]^2, df=1)
pc1_pvalue
pc2_pvalues
pc2_pvalue
ryan <- matrix(data=NA, nrow=num_GOF, ncol=3)
for (sim_it in 1:num_GOF)
{
# Start with G
G_vec <- rbinom(n=num_sub, size=2, prob=0.3)
# Center and scale G so that sum(G_i^2) =1
G_vec <- ( G_vec-mean(G_vec) )
sum_G2 <- sum(G_vec^2)
G_vec <- G_vec / sqrt(sum_G2)
# Then generate Y
Y_mat <- matrix(data=NA, nrow=num_sub, ncol=K)
for (i in 1:num_sub) {
Y_mat[i, ] <- rmvn(n=1, mu=(beta_vec*G_vec[i]), sigma=sig_mat_exch)
}
# Then standardize Y
for (i in 1:ncol(Y_mat)) {
temp_col <- Y_mat[, i]
Y_mat[, i] <-  (temp_col - mean(temp_col)) / sd(temp_col)
}
# Now build our test statistics
# A score test is t(G) %*% Y
test_stats <- t(G_vec) %*% Y_mat
# Eigenvectors and eigenvalues
Y_sig <- cor(Y_mat)
SM_eigen <- eigen(Y_sig)
U <- SM_eigen$vectors
Lambda <- SM_eigen$values
# Testing on PCs
all_PC_test_stats <- test_stats %*% U
PC_test_stats_stan <- all_PC_test_stats / sqrt(Lambda)
pc1_pvalue <- 1-pchisq(PC_test_stats_stan[1]^2, df=1)
pc2_pvalue <- 1-pchisq(PC_test_stats_stan[2]^2, df=1)
pcK_pvalue <- 1-pchisq(PC_test_stats_stan[K]^2, df=1)
ryan[sim_it, 1] <- PC_test_stats_stan[1]
ryan[sim_it, 2] <- PC_test_stats_stan[2]
ryan[sim_it, 3] <- PC_test_stats_stan[3]
# VC test
VC_stat <- (test_stats %*% temp_inv %*% temp_inv %*% t(test_stats))^2
# Now apply Cholesky decomposition
cee <- solve(t(chol(Y_sig)))
chol_vec <- cee %*% t(test_stats)
# Apply BJ and HC to both the Cholesky decomped vector and the PC test stats vector
BJ_chol <- BJ(test_stats=chol_vec, cor_mat=diag(nrow=K))
BJ_PC <- BJ(test_stats=PC_test_stats_stan, cor_mat=diag(nrow=K))
HC_chol <- HC(test_stats=chol_vec, cor_mat=diag(nrow=K))
HC_PC <- HC(test_stats=PC_test_stats_stan, cor_mat=diag(nrow=K))
# Apply GBJ and GHC to the original test stats
GBJ_orig <- GBJ(test_stats=test_stats, cor_mat=Y_sig)
GHC_orig <- GHC(test_stats=test_stats, cor_mat=Y_sig)
# Record
record_row <- (num_causal-1)*num_GOF + sim_it
mult_pheno_exch[record_row, 1] <- pc1_pvalue
mult_pheno_exch[record_row, 2] <- pc2_pvalue
mult_pheno_exch[record_row, 3] <- pcK_pvalue
mult_pheno_exch[record_row, 4] <- as.numeric(VC_stat > VC_crit_val)
mult_pheno_exch[record_row, 5] <- BJ_chol$BJ_pvalue
mult_pheno_exch[record_row, 6] <- BJ_PC$BJ_pvalue
mult_pheno_exch[record_row, 7] <- HC_chol$HC_pvalue
mult_pheno_exch[record_row, 8] <- HC_PC$HC_pvalue
mult_pheno_exch[record_row, 9] <- GBJ_orig$GBJ_pvalue
mult_pheno_exch[record_row, 10] <- GHC_orig$GHC_pvalue
cat(sim_it)
}
head(ryan)
mean(ryan[,1], na.rm=TRUE)
mean(ryan[,2], na.rm=TRUE)
mean(ryan[,3], na.rm=TRUE)
test_stats
head(all_PC_test_stats)
all_PC_test_stats[1:5,1:5]
all_PC_test_stats[,1:5]
head(Lambda)
PC_test_stats_stan[,1:5]
3.923009 / sqrt(31)
1.287899 / sqrt(1.16)
getwd()
document()
library(roxygen2)
library(devtools)
document()
document()
build()
build()
getwd()
setwd('/Users/ryansun/Documents/Research/Paper2/Software')
install.packages('GBJ_0.1.0.tar.gz', type='source', repos=NULL)
library(GBJ)
set.seed(1000)
cancer_status <- c(rep(1,100), rep(0,100))
# All of our SNPs have minor allele frequency of 0.3 in this example
genotype_data <- matrix(data=rbinom(n=200*50, size=2, prob=0.3), nrow=200)
age <- round( runif(n=200, min=30, max=80) )
gender <- rbinom(n=200, size=1, prob=0.5)     # Let 1 denote a female and 0 a male
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, link_function="logistic")
log_reg_stats$test_stats
log_reg_stats$cor_mat[1:5,1:5]
cor_Z <- log_reg_stats$cor_mat
score_stats = log_reg_stats$test_stats
GBJ(test_stats=score_stats, cor_mat=cor_Z)
GHC(test_stats=score_stats, cor_mat=cor_Z)
HC(test_stats=score_stats, cor_mat=cor_Z)
BJ(test_stats=score_stats, cor_mat=cor_Z)
minP(test_stats=score_stats, cor_mat=cor_Z)
score_stats
score_stats[1:10] <- 8
GBJ(test_stats=score_stats, cor_mat=cor_Z)
GHC(test_stats=score_stats, cor_mat=cor_Z)
HC(test_stats=score_stats, cor_mat=cor_Z)
BJ(test_stats=score_stats, cor_mat=cor_Z)
minP(test_stats=score_stats, cor_mat=cor_Z)
genotype_data
dim(genotype_data)
genotyp_data[,5] <- 1
genotype_data[,5] <- 1
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, link_function="logistic")
genotype_data[,5]
log_reg_stats
log_reg_stats$test_stats
factor_matrix = genotype_data
link_function = 'logistic'
null_model = null_mod
d <- ncol(factor_matrix)
test_stats <- rep(NA, d)
denominators <- rep(NA, d)
kkk=5
tempF <- factor_matrix[,kkk]
score_num <- t(tempF) %*% (actual_Y-fitted_Y)
score_denom <- sqrt(tempF %*% P_mat %*% tempF)
X_mat <- model.matrix(null_model)
fitted_Y <- null_model$fitted.values
actual_Y <- null_model$y
# Only difference between linear and logistic procedure
if (link_function == 'logistic') {
W_vec <- fitted_Y * (1-fitted_Y)
} else if (link_function == 'linear') {
W_vec <- rep(summary(null_model)$dispersion, nrow(X_mat))
} else if (link_function == 'log') {
W_vec <- fitted_Y
} else {
stop("Invalid model type")
}
# Regular mode
W_mat <- diag(W_vec)
P_mat <- W_mat - W_mat%*%X_mat %*% solve(t(X_mat)%*%W_mat%*%X_mat) %*% t(X_mat)%*%W_mat
d <- ncol(factor_matrix)
test_stats <- rep(NA, d)
denominators <- rep(NA, d)
tempF <- factor_matrix[,kkk]
score_num <- t(tempF) %*% (actual_Y-fitted_Y)
score_denom <- sqrt(tempF %*% P_mat %*% tempF)
kkk
score_denom]
score_denom
dim(tempF)
length(tempF)
P_mat <- diag(nrow=200) - matrix(data=1/200, nrow=200, ncol=200)
score_denom <- sqrt(tempF %*% P_mat %*% tempF)
(tempF %*% P_mat %*% tempF)
class(score_denom)
score_denom
score_denom <- tryCatch(sqrt(tempF %*% P_mat %*% tempF), warning=function(w) w,
error=function(e) e)
score_dneom
score_denom
class(score_denom)
score_denom <- tryCatch(sqrt(tempF %*% P_mat %*% tempF), warning=function(w) w,
error=function(e) e)
# We've been getting negative denominators with, for example, very rare SNPs
if (length(class(score_denom)) > 1) {
err_msg <- paste('Error in calculating test statistic for factor ', kkk,
' possibly it is constant?  Try removing and rerunning.', sep='')
stop(err_msg)
}
kkk = 2
tempF <- factor_matrix[,kkk]
score_num <- t(tempF) %*% (actual_Y-fitted_Y)
score_denom <- tryCatch(sqrt(tempF %*% P_mat %*% tempF), warning=function(w) w,
error=function(e) e)
# We've been getting negative denominators with, for example, very rare SNPs
if (length(class(score_denom)) > 1) {
err_msg <- paste('Error in calculating test statistic for factor ', kkk,
' - possibly it is constant?  Try removing and rerunning.', sep='')
stop(err_msg)
}
score_denom
getwd()
setwd('/Users/ryansun/Documents/Research/Paper2/Software/GBJ')
build()
library(devtools)
build()
setwd('/Users/ryansun/Documents/Research/Paper2/Software')
install.packages('GBJ_0.1.1.tar.gz', repos=NULL, type='source')
library(GBJ)
set.seed(1000)
cancer_status <- c(rep(1,100), rep(0,100))
# All of our SNPs have minor allele frequency of 0.3 in this example
genotype_data <- matrix(data=rbinom(n=200*50, size=2, prob=0.3), nrow=200)
age <- round( runif(n=200, min=30, max=80) )
gender <- rbinom(n=200, size=1, prob=0.5)
null_mod <- glm(cancer_status~age+gender, family=binomial(link="logit"))
log_reg_stats <- calc_score_stats(null_model=null_mod, factor_matrix=genotype_data, link_function="logistic")
log_reg_stats$test_stats
log_reg_stats$cor_mat[1:5,1:5]
cor_Z <- log_reg_stats$cor_mat
score_stats = log_reg_stats$test_stats
GBJ(test_stats=score_stats, cor_mat=cor_Z)
GHC(test_stats=score_stats, cor_mat=cor_Z)
HC(test_stats=score_stats, cor_mat=cor_Z)
BJ(test_stats=score_stats, cor_mat=cor_Z)
minP(test_stats=score_stats, cor_mat=cor_Z)
test_stats[1:10] <- 8
score_stats[1:10] <- 10
GBJ(test_stats=score_stats, cor_mat=cor_Z)
BJ(test_stats=score_stats, cor_mat=cor_Z)
minP(test_stats=score_stats, cor_mat=cor_Z)
pnorm(8)
qnorm(8)
pnorm(7.5)
1-pnorm(8)
1-pnorm(8.2)
build()
getwd()
library(devtools)
getwd()
build()
setwd('/Users/ryansun/Documents/Research/Paper2/Software/GBJ')
build()
build()
